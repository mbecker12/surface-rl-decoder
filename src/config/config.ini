[env]
size=5
min_qbit_err=0
p_error=0.01
p_msmt=0.01
stack_depth=5
error_channel=dp
max_actions=24
state_multiplier=1
terminal_action=4

[general]
summary_path=testing_2d
summary_date=test
summary_run_info=run_info
description=

[actor]
num_cuda=0
num_cpu=2
num_environments=16
size_local_memory_buffer=1000
verbosity=2
benchmarking=0
epsilon=0.8
load_model=0
discount_intermediate_reward=0.3
min_value_factor_intermediate_reward=0.0
decay_factor_intermediate_reward=0.99997
decay_factor_epsilon=0.99997
min_value_factor_epsilon=0.0

[replay_memory]
size=100000
replay_size_before_sampling=100
verbosity=2
benchmarking=0
memory_type=prio
alpha=0.9
beta=0.6
decay_beta=1.00002
nvidia_log_frequency=15

;ppo
episode_buffer_tau=0.97
max_buffer_episodes=512
max_buffer_episode_steps=25
episode_buffer_device=cpu

[learner]
verbosity=6
benchmarking=0
; hours
max_time_h=2
; minutes
max_time_minutes=30
learning_rate=5e-3
device=cpu
target_update_steps=50
discount_factor=0.95
learner_epsilon=0.0
batch_size=32
eval_frequency=20
max_timesteps=1000000
; model_name=dummy_agent
; model_config_file=dummy_agent.json
model_name=conv2d
; only has effect for 2D conv for now
transfer_learning=1
model_config_file=conv_agents_slim.json
model_config_location=src/config/model_spec/
load_model=0
load_model_path=networks/test13/dummy_agent_5_test13.pt
save_model_path=networks
base_model_config_path=src/config/model_spec/old_conv_agents.json
base_model_path=remote_networks/5/65280/simple_conv_5_65280.pt

;ppo
policy_model_max_grad_norm=1000
policy_clip_range=0.1
policy_stopping_kl=0.02
value_model_max_grad_norm=1000
value_clip_range=0.1
value_stopping_mse=25
entropy_loss_weight=0.01
value_loss_weight=0.0001
max_episodes=32
optimization_epochs=10

[reward]
non_trivial_loop=-39
syndrome_left=-19
solved_episode=100
syndrome_difference=5
repeating_action=0
premature_ending=-200
