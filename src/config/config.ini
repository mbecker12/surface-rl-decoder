[env]
size=5
min_qbit_err=1
p_error=0.01
p_msmt=0.00
stack_depth=8
error_channel=iidxz
max_actions=128
state_multiplier=1
terminal_action=4

[general]
summary_path=testing_ppo
summary_date=eval
summary_run_info=run_info
description=

[actor]
num_cuda=0
num_cpu=4
num_environments=8
size_local_memory_buffer=1000
verbosity=2
benchmarking=0
epsilon=0.8
load_model=0
discount_intermediate_reward=0.3
min_value_factor_intermediate_reward=0.0
decay_factor_intermediate_reward=0.99997
decay_factor_epsilon=0.99997
min_value_factor_epsilon=0.0

[replay_memory]
size=100000
replay_size_before_sampling=1000
verbosity=2
benchmarking=0
memory_type=prio
alpha=0.9
beta=0.4
decay_beta=1.00002
nvidia_log_frequency=15

;ppo
episode_buffer_tau=0.97
max_buffer_episodes=512
max_buffer_episode_steps=25
episode_buffer_device=cpu

[learner]
verbosity=5
benchmarking=0
; hours
max_time_h=0
; minutes
max_time_minutes=10
learning_rate=1e-3
device=cpu
target_update_steps=200
discount_factor=0.95
learner_epsilon=0.0
batch_size=8
eval_frequency=25
max_timesteps=1000000
; model_name=dummy_agent
; model_config_file=dummy_agent.json
model_name=conv3d
model_config_file=conv_agents_slim.json
model_config_location=src/config/model_spec/
load_model=0
load_model_path=networks/test13/dummy_agent_5_test13.pt
save_model_path=networks

;ppo
policy_model_max_grad_norm=10000
policy_clip_range=0.1
policy_stopping_kl=0.02
value_model_max_grad_norm=10000
value_clip_range=100
value_stopping_mse=25
entropy_loss_weight=0.1
value_loss_weight=0.8
max_episodes=32
optimization_epochs=10

[reward]
non_trivial_loop=-39
syndrome_left=-19
solved_episode=100
syndrome_difference=5
repeating_action=0
